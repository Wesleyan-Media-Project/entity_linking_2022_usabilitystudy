{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ZCFMuu0TYg"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmPosWh3IlLG"
      },
      "source": [
        "### **This section must be run before attempting to execute any other sections' code cells in order to ensure that all proper packages are installed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "291NP1oQcied",
        "outputId": "1b53913f-ce9b-49e6-b4dd-e48918dbb9f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version is already 3.10 (3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Check Python version\n",
        "current_version = sys.version_info\n",
        "\n",
        "if current_version.major != 3 or current_version.minor != 10:\n",
        "    print(f\"Current Python version is {current_version.major}.{current_version.minor}.\")\n",
        "    print(\"Switching to Python 3.10. Please wait...\")\n",
        "\n",
        "    # Install Python 3.10\n",
        "    !sudo apt-get update\n",
        "    !sudo apt-get install python3.10 python3.10-distutils -y\n",
        "\n",
        "    # Install virtualenv if not installed\n",
        "    !pip install virtualenv\n",
        "\n",
        "    # Create a virtual environment using Python 3.10\n",
        "    !virtualenv -p python3.10 py310_env\n",
        "    !source py310_env/bin/activate\n",
        "\n",
        "    print(\"Python 3.10 environment is ready. Restart the notebook and activate the virtual environment to use it.\")\n",
        "else:\n",
        "    print(f\"Python version is already 3.10 ({sys.version}).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rWuFfV-ZdGK0"
      },
      "outputs": [],
      "source": [
        "!pip3 install spacy==3.2.4\n",
        "!pip3 install numpy==1.26.4\n",
        "!pip3 install pandas==2.1.1\n",
        "\n",
        "# To check versions uncomment:\n",
        "#!pip3 list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "B5yJelC6bdaP",
        "outputId": "e1d755d7-032d-44f2-e076-ae7b7f645e96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from rpy2) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from rpy2) (3.1.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from rpy2) (2024.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from rpy2) (5.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10.0->rpy2) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->rpy2) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install rpy2\n",
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rDqsLG-jd1TN"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "install.packages(\"dplyr\")\n",
        "install.packages(\"data.table\")\n",
        "install.packages(\"stringr\")\n",
        "install.packages(\"tidyr\")\n",
        "install.packages(\"R.utils\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounts Google Drive so you can import external dependencies (e.g., trained_entity_linker.zip)\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pu0Xi6TlkSPx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8KUgcZeNK3-"
      },
      "source": [
        "### Note: If you have uploaded the **pre-trained entity linker model** into Google Drive for use with this code (```trained_entity_linker.zip```), you can now move directly to the inference section of the notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw1nuufm0ZrV"
      },
      "source": [
        "# knowledge_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l_qtnpnB3hB"
      },
      "source": [
        "Steps to take before attempting to run knowledge_base:\n",
        "\n",
        "- **Make sure that ```person_2022.csv```, ```wmpcand_120223_wmpid``` and ```bp2022_house_scraped_face_jasmine.xlsx``` are uploaded onto Google Drive. If they are not directly located in your main drive (e.g, they are in a folder) then the paths used in the code cells must be updated.**\n",
        "\n",
        "  - Files [```person_2022.csv```](https://github.com/Wesleyan-Media-Project/datasets/blob/main/people/person_2022.csv), [```wmpcand_120223_wmpid```](https://github.com/Wesleyan-Media-Project/datasets/blob/main/candidates/wmpcand_120223_wmpid.csv) and [```bp2022_house_scraped_face_jasmine.xlsx```](https://github.com/Wesleyan-Media-Project/face_url_scraper_2022/blob/main/data/bp2022_house_scraped_face_jasmine.xlsx) can be downloaded/uploaded using the links provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ikPrFHG0rlK"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "library(dplyr)\n",
        "library(haven)\n",
        "library(data.table)\n",
        "library(stringr)\n",
        "library(quanteda)\n",
        "library(readxl)\n",
        "library(tidyr)\n",
        "\n",
        "setwd(\"./\")\n",
        "\n",
        "# File paths\n",
        "# In\n",
        "\n",
        "# These files are located in our datasets repository (https://github.com/Wesleyan-Media-Project/datasets)\n",
        "# Make sure that these files are uploaded into the colab environment before attempting to run\n",
        "\n",
        "path_people_file <- \"/content/drive/MyDrive/person_2022.csv\"\n",
        "path_cand_file <- \"/content/drive/MyDrive/wmpcand_120223_wmpid.csv\"\n",
        "# Out\n",
        "path_kb <- \"entity_kb.csv\"\n",
        "\n",
        "\n",
        "# People file\n",
        "people <- fread(path_people_file, encoding = \"UTF-8\", data.table = F)\n",
        "# Create some additional person categories\n",
        "people$pubhealth <- ifelse(people$face_category == \"public health related\", 1, 0)\n",
        "people$cabinet <- ifelse(people$face_category == \"cabinet\", 1, 0)\n",
        "people$historical <- ifelse(people$face_category == \"historical figures\", 1, 0)\n",
        "# In case any of these variables contain NAs (they largely don't any more)\n",
        "# Make them 0s instead\n",
        "people$supcourt_2022[is.na(people$supcourt_2022)] <- 0\n",
        "people$supcourt_former[is.na(people$supcourt_former)] <- 0\n",
        "people$currsen_2022[is.na(people$currsen_2022)] <- 0\n",
        "people$prompol[is.na(people$prompol)] <- 0\n",
        "people$former_uspres[is.na(people$former_uspres)] <- 0\n",
        "people$intl_leaders[is.na(people$intl_leaders)] <- 0\n",
        "people$gov2022_gencd[is.na(people$gov2022_gencd)] <- 0\n",
        "\n",
        "# Make sure there are no duplicate people\n",
        "if (any(duplicated(people$wmpid))) {\n",
        "  stop(\"There are duplicate people.\")\n",
        "}\n",
        "\n",
        "\n",
        "# Candidate file\n",
        "# Make sure that genelect is 1 so we ignore duplicate versions of the same candidate who ran for different offices but only made it to the general election in one\n",
        "# Also retain only relevant variables\n",
        "cands <- fread(path_cand_file, encoding = \"UTF-8\", data.table = F)\n",
        "cands <- cands %>%\n",
        "  filter(genelect_cd == 1) %>%\n",
        "  select(wmpid, genelect_cd, cand_id, cand_office, cand_office_st, cand_office_dist, cand_party_affiliation)\n",
        "# Make sure there are no duplicate candidates\n",
        "if (any(duplicated(cands$wmpid))) {\n",
        "  stop(\"There are duplicate candidates.\")\n",
        "}\n",
        "\n",
        "# Merge candidate file into people file\n",
        "people <- left_join(people, cands, by = \"wmpid\")\n",
        "\n",
        "# Restrict to only 2022 candidates and other relevant people\n",
        "# Also retain only relevant variables\n",
        "people <- people %>%\n",
        "  filter(genelect_cd == 1 | supcourt_2022 == 1 | supcourt_former == 1 | currsen_2022 == 1 | prompol == 1 | former_uspres == 1 | intl_leaders == 1 | gov2022_gencd == 1 | pubhealth == 1 | cabinet == 1 | historical == 1) %>%\n",
        "  select(wmpid, full_name, first_name, last_name, fecid_2022a, fecid_2022b, genelect_cd, supcourt_2022, supcourt_former, currsen_2022, prompol, former_uspres, intl_leaders, gov2022_gencd, pubhealth, cabinet, historical, cand_id, cand_office, cand_office_st, cand_office_dist, cand_party_affiliation)\n",
        "\n",
        "\n",
        "entities_candidate <- people$full_name\n",
        "\n",
        "tks <- tokens(entities_candidate)\n",
        "#---- FIRST NAME\n",
        "# the first word is always the first name\n",
        "people$first_name_extracted <- unlist(lapply(tks, function(x) {\n",
        "  x[1]\n",
        "}))\n",
        "\n",
        "#---- LAST NAME\n",
        "# If the name consists of two words, then the second one is the last name\n",
        "people$last_name_extracted <- unlist(lapply(tks, function(x) {\n",
        "  if (length(x) == 2) {\n",
        "    x[2]\n",
        "  } else {\n",
        "    NA\n",
        "  }\n",
        "}))\n",
        "# If the name consists of more than two words, then the last one is the last name\n",
        "last_name_temp <- unlist(lapply(tks, function(x) {\n",
        "  if (length(x) > 2) {\n",
        "    x[length(x)]\n",
        "  } else {\n",
        "    NA\n",
        "  }\n",
        "}))\n",
        "people$last_name_extracted[is.na(last_name_temp) == F] <- last_name_temp[is.na(last_name_temp) == F]\n",
        "# if the last word is jr or sr, the second-to last word is the last name\n",
        "last_word_temp <- unlist(lapply(tks, function(x) {\n",
        "  x[length(x)]\n",
        "}))\n",
        "jr_temp_indices <- which(last_word_temp %in% c(\".\", \"Jr\", \"Sr\"))\n",
        "jr_temp_names <- entities_candidate[jr_temp_indices]\n",
        "jr_temp_suffix <- str_extract(jr_temp_names, \"[J|S]r\")\n",
        "people$suffix_name_extracted <- NA\n",
        "people$suffix_name_extracted[jr_temp_indices] <- jr_temp_suffix\n",
        "jr_temp_names_without_suffix <- str_remove(jr_temp_names, \" [J|S]r.?\") # remove Jr/Sr + 0 or more occurence of .\n",
        "jr_temp_names_without_suffix_tks <- tokens(jr_temp_names_without_suffix)\n",
        "jr_temp_last_names <- unlist(lapply(jr_temp_names_without_suffix_tks, function(x) {\n",
        "  x[length(x)]\n",
        "}))\n",
        "people$last_name_extracted[jr_temp_indices] <- jr_temp_last_names\n",
        "\n",
        "\n",
        "# the II, the III\n",
        "II_temp_indices <- which(last_word_temp %in% c(\"II\", \"III\"))\n",
        "II_temp_names <- entities_candidate[II_temp_indices]\n",
        "II_temp_suffix <- str_extract(II_temp_names, \"II+\")\n",
        "people$suffix_name_extracted[II_temp_indices] <- II_temp_suffix\n",
        "II_temp_names_without_suffix <- str_remove(II_temp_names, \" II+\") # remove II/III\n",
        "II_temp_names_without_suffix_tks <- tokens(II_temp_names_without_suffix)\n",
        "II_temp_last_names <- unlist(lapply(II_temp_names_without_suffix_tks, function(x) {\n",
        "  x[length(x)]\n",
        "}))\n",
        "people$last_name_extracted[II_temp_indices] <- II_temp_last_names\n",
        "\n",
        "#---- MIDDLE NAMES\n",
        "name_len <- unlist(lapply(tks, length))\n",
        "no_middle_name <- which(name_len == 2)\n",
        "no_middle_name <- sort(unique(c(no_middle_name, jr_temp_indices, II_temp_indices)))\n",
        "middle_name_indices <- (1:nrow(people))[which((1:nrow(people) %in% no_middle_name) == F)] # people who do have middle names\n",
        "tks_middle_names <- tks[middle_name_indices]\n",
        "tks_middle_names <- lapply(tks_middle_names, function(x) {\n",
        "  x[-1]\n",
        "}) # remove the first word\n",
        "tks_middle_names <- lapply(tks_middle_names, function(x) {\n",
        "  x[-length(x)]\n",
        "}) # remove the last word\n",
        "tks_middle_names <- lapply(tks_middle_names, paste0, collapse = \" \") # combine them and make a space so that multiple middle names, or \"De La\" etc. get resolved\n",
        "tks_middle_names <- str_replace_all(tks_middle_names, \" \\\\.\", \"\\\\.\") # this does create a problem with periods, clean them up\n",
        "tks_middle_names <- str_replace(tks_middle_names, \"^\\\\.\", \"\") # remove periods if they are the first char\n",
        "tks_middle_names <- str_trim(tks_middle_names) # clean up spaces at beginning/end\n",
        "people$middle_name_extracted <- NA\n",
        "people$middle_name_extracted[middle_name_indices] <- tks_middle_names\n",
        "people$middle_name_extracted[which(people$middle_name_extracted == \"\")] <- NA # some people ended up with an empty middle name, remove\n",
        "\n",
        "\n",
        "# ----\n",
        "# JASMINE'S FIXES to candidate names\n",
        "# This file is located in our face_url_scraper_2022 repository (https://github.com/Wesleyan-Media-Project/face_url_scraper_2022)\n",
        "# Make sure the face_url_scraper_2022 folder is located in the same directory as entity_linking_2022\n",
        "fixes <- read_xlsx(\"/content/bp2022_house_scraped_face_jasmine.xlsx\") # nolint: line_length_linter.\n",
        "fixes <- fixes %>%\n",
        "  select(wmpid, cand_name, full_name, starts_with(\"hc\")) %>%\n",
        "  select(-c(hc_face_note, hc_face_url, hc_office_district, hc_office_district_note))\n",
        "\n",
        "people <- left_join(people, fixes, by = \"wmpid\")\n",
        "\n",
        "# Overwrite with Jasmine's fixes\n",
        "people$first_name_extracted[is.na(people$hc_first_name) == F] <- people$hc_first_name[is.na(people$hc_first_name) == F]\n",
        "people$middle_name_extracted[is.na(people$hc_middle_name) == F] <- people$hc_middle_name[is.na(people$hc_middle_name) == F]\n",
        "people$last_name_extracted[is.na(people$hc_last_name) == F] <- people$hc_last_name[is.na(people$hc_last_name) == F]\n",
        "people$suffix_name_extracted[is.na(people$hc_suffix) == F] <- people$hc_suffix[is.na(people$hc_suffix) == F]\n",
        "\n",
        "# Correct names\n",
        "people$first_name <- people$first_name_extracted\n",
        "people$middle_name <- people$middle_name_extracted\n",
        "people$last_name <- people$last_name_extracted\n",
        "people$suffix_name <- people$suffix_name_extracted\n",
        "people <- unite(people, \"full_name\", c(first_name, middle_name, last_name, suffix_name), sep = \" \", na.rm = T, remove = F)\n",
        "people <- unite(people, \"full_name_first_last\", c(first_name, last_name), sep = \" \", na.rm = T, remove = F)\n",
        "people$full_name <- str_squish(people$full_name)\n",
        "people$full_name_first_last <- str_squish(people$full_name_first_last)\n",
        "\n",
        "# ----\n",
        "# CANDIDATE DESCRIPTIONS\n",
        "# Party\n",
        "people$party[!people$cand_party_affiliation %in% c(\"DEM\", \"REP\")] <- \"3rd party\"\n",
        "people$party[people$cand_party_affiliation == \"DEM\"] <- \"Democratic\"\n",
        "people$party[people$cand_party_affiliation == \"REP\"] <- \"Republican\"\n",
        "people$party[is.na(people$cand_party_affiliation)] <- NA\n",
        "\n",
        "# District number\n",
        "district_number <- as.character(as.numeric(people$cand_office_dist))\n",
        "district_number <- str_replace(district_number, \"$\", \"th\")\n",
        "district_number <- str_replace(district_number, \"1th\", \"1st\")\n",
        "district_number <- str_replace(district_number, \"2th\", \"2nd\")\n",
        "district_number <- str_replace(district_number, \"3th\", \"3rd\")\n",
        "district_number <- str_replace(district_number, \"11st\", \"11th\")\n",
        "district_number <- str_replace(district_number, \"12nd\", \"12th\")\n",
        "\n",
        "# State name rather than abbreviation\n",
        "state_name <- state.name[match(people$cand_office_st, state.abb)]\n",
        "\n",
        "# Construct the descriptions\n",
        "people$descr <- NA\n",
        "for (i in 1:nrow(people)) {\n",
        "  if (is.na(people$genelect_cd[i]) == F) {\n",
        "    if (people$cand_office[i] == \"H\") {\n",
        "      people$descr[i] <- paste0(people$full_name[i], \" is a \", people$party[i], \" candidate for the \", district_number[i], \" District of \", state_name[i], \".\")\n",
        "    } else if (people$cand_office[i] == \"S\") {\n",
        "      people$descr[i] <- paste0(people$full_name[i], \" is a \", people$party[i], \" Senate candidate in \", state_name[i], \".\")\n",
        "    }\n",
        "  } else if (people$currsen_2022[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is a Senator.\")\n",
        "  } else if (people$former_uspres[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is a former U.S. president.\")\n",
        "  } else if (people$prompol[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is a prominent politician.\")\n",
        "  } else if (people$intl_leaders[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is an international leader.\")\n",
        "  } else if (people$supcourt_2022[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is a Supreme Court Justice.\")\n",
        "  } else if (people$supcourt_former[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is a former Supreme Court Justice.\")\n",
        "  } else if (people$gov2022_gencd[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is a gubernatorial candidate.\")\n",
        "  } else if (people$pubhealth[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is a public health official.\")\n",
        "  } else if (people$cabinet[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is a cabinet member.\")\n",
        "  } else if (people$historical[i] == 1) {\n",
        "    people$descr[i] <- paste0(people$full_name[i], \" is a historical figure.\")\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "# ----\n",
        "# Candidate aliases\n",
        "for (i in 1:nrow(people)) {\n",
        "  cand_names <- c(people$full_name[i], people$last_name[i], people$full_name_first_last[i])\n",
        "  if (substr(cand_names[1], nchar(cand_names[1]), nchar(cand_names[1])) != \"s\") {\n",
        "    cand_aliases <- c(cand_names, paste0(cand_names, \"'s\"))\n",
        "  } else {\n",
        "    cand_aliases <- c(cand_names, paste0(cand_names, \"'\"))\n",
        "  }\n",
        "  cand_aliases <- c(cand_aliases, toupper(cand_aliases))\n",
        "\n",
        "  people$aliases[[i]] <- c(cand_aliases)\n",
        "}\n",
        "\n",
        "# ----\n",
        "# Create knowledge base\n",
        "kb <- people %>%\n",
        "  select(wmpid, full_name, descr, aliases) %>%\n",
        "  rename(id = wmpid, name = full_name)\n",
        "\n",
        "# One-off fixes\n",
        "kb$descr[kb$id == \"WMPID1289\"] <- \"Joe Biden is the U.S. president.\"\n",
        "kb$aliases[[1107]] <- str_remove(kb$aliases[[1107]], \",\") # Remove commas from MLK because it screws with the csv\n",
        "\n",
        "# Make sure every alias only exists once (people without middle names or suffixes will have duplicates otherwise)\n",
        "kb$aliases <- lapply(kb$aliases, unique)\n",
        "\n",
        "fwrite(kb, path_kb)\n",
        "# The 4 variables in this file are the only thing\n",
        "# from this script that enter the entity linker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slUEB4EI0OO4"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCkuvW02DWbW"
      },
      "source": [
        "Steps to take before attempting to run train:\n",
        "\n",
        "- **Make sure that ```fb_2022_adid_text.csv.gz```, ```fb_2022_adid_var1.csv.gz```, ```entity_kb.csv``` and ```wmp_fb_2022_entities_v082324.csv``` are uploaded onto Google Drive. If they are not directly located in your main drive (e.g, they are in a folder) then the paths used in the code cells must be updated..**\n",
        "\n",
        "  - Files ```fb_2022_adid_text.csv.gz``` and ```fb_2022_adid_var1.csv.gz``` must be downloaded from our Figshare page. You can get access using this [Data Access form](https://www.creativewmp.com/data-access/).\n",
        "\n",
        "  - File [```entity_kb.csv```](https://github.com/Wesleyan-Media-Project/entity_linking_2022_usabilitystudy/blob/main/facebook/data/entity_kb.csv) will either already be present as output from the knowledge_base section, or you can download/upload it manually from the link provided. If it is present as output from the knowledge_base section then the path must be changed as specified in the code cells.\n",
        "\n",
        "  - File [```wmp_fb_2022_entities_v082324.csv```](https://github.com/Wesleyan-Media-Project/datasets/blob/main/wmp_entity_files/Facebook/wmp_fb_2022_entities_v082324.csv) can be downloaded/uploaded manually from the link provided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSebKtSX6M41"
      },
      "outputs": [],
      "source": [
        "!python3 -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7RBV2hdzhH5"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "library(data.table)\n",
        "library(dplyr)\n",
        "library(tidyr)\n",
        "\n",
        "setwd(\"./\")\n",
        "\n",
        "# Input files\n",
        "# This is an output from data-post-production/01-merge-results/01_merge_preprocessed_results\n",
        "# Select fields of 'ad_id', 'page_name', 'disclaimer', 'ad_creative_body',\n",
        "#        'ad_creative_link_caption', 'ad_creative_link_title',\n",
        "#        'ad_creative_link_description', 'aws_ocr_text_img',\n",
        "#        'google_asr_text', 'aws_ocr_text_vid'\n",
        "#############################################################################################\n",
        "\n",
        "# Make sure that these files are in the colab environment before attempting to run\n",
        "\n",
        "path_ads <- \"/content/drive/MyDrive/fb_2022_adid_text.csv.gz\"\n",
        "path_adid_to_pageid <- \"/content/drive/MyDrive/fb_2022_adid_var1.csv.gz\"\n",
        "\n",
        "# If you ran the knowledge_base section, this path must be changed to \"/content/entity_kb.csv\"\n",
        "path_entities_kb <- \"/content/drive/MyDrive/entity_kb.csv\"\n",
        "\n",
        "# This file is located in our datasets repository (https://github.com/Wesleyan-Media-Project/datasets)\n",
        "# Make sure that these files are uploaded into the colab environment before attempting to run\n",
        "\n",
        "path_wmpent_file <- \"/content/drive/MyDrive/wmp_fb_2022_entities_v082324.csv\" # nolint: line_length_linter.\n",
        "# Output files\n",
        "path_output <- \"ads_with_aliases.csv.gz\"\n",
        "\n",
        "# Pdid to wmpid\n",
        "wmpents <- fread(path_wmpent_file) %>%\n",
        "  select(pd_id, wmpid)\n",
        "wmpents <- wmpents[wmpents$wmpid != \"\", ]\n",
        "\n",
        "# Ads\n",
        "df <- fread(path_ads, encoding = \"UTF-8\")\n",
        "\n",
        "cols <- c(\n",
        "  \"ad_id\", \"page_name\", \"disclaimer\", \"ad_creative_body\", \"ad_creative_link_caption\", \"ad_creative_link_title\",\n",
        "  \"ad_creative_link_description\", \"aws_ocr_text_img\", \"google_asr_text\", \"aws_ocr_text_vid\"\n",
        ") # nolint\n",
        "# Select only the specified columns\n",
        "df <- df[, ..cols]\n",
        "\n",
        "# Adid to pdid\n",
        "adid_to_pageid <-\n",
        "  fread(path_adid_to_pageid, colClasses = \"character\") %>%\n",
        "  select(ad_id, pd_id)\n",
        "\n",
        "# Combine\n",
        "df <- inner_join(df, adid_to_pageid, by = \"ad_id\")\n",
        "df <- left_join(df, wmpents, by = \"pd_id\")\n",
        "\n",
        "# Aliases, then merge in pd_id\n",
        "aliases <- fread(path_entities_kb, encoding = \"UTF-8\", data.table = F)\n",
        "aliases <- select(aliases, c(id, aliases))\n",
        "\n",
        "# Keep only ads that have a wmpid\n",
        "# Shape to long format\n",
        "# Remove empty rows\n",
        "# Keep only distinct rows based on pd_id and value\n",
        "df <- df %>%\n",
        "  filter(wmpid != \"\") %>%\n",
        "  pivot_longer(-c(ad_id, pd_id, wmpid)) %>%\n",
        "  filter(value != \"\") %>%\n",
        "  distinct_at(vars(pd_id, value), .keep_all = T)\n",
        "\n",
        "# Merge in aliases\n",
        "df <- left_join(df, aliases, by = c(\"wmpid\" = \"id\"))\n",
        "\n",
        "# Get rid of ads that have no aliases\n",
        "df <- df[is.na(df$aliases) == F, ]\n",
        "\n",
        "fwrite(df, path_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP7a8kkdzzXC"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import spacy # Use version 3.2.4\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "from spacy.kb import KnowledgeBase #vscode pylinter complains, actually loads fine\n",
        "# for spacy version above v3.5\n",
        "# from spacy.kb import InMemoryLookupKB\n",
        "from spacy.util import minibatch, compounding\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from spacy.training import Example\n",
        "from spacy.ml.models import load_kb\n",
        "\n",
        "\n",
        "# Input files\n",
        "# Make sure that these files are in the colab environment before attempting to run\n",
        "\n",
        "# If you ran the knowledge_base section, this path must be changed to \"/content/entity_kb.csv\"\n",
        "path_candidates = \"/content/drive/MyDrive/entity_kb.csv\"\n",
        "\n",
        "path_training_samples = \"/content/ads_with_aliases.csv.gz\"\n",
        "\n",
        "# Output files\n",
        "path_intermediate_kb = \"intermediate_kb\"\n",
        "path_output_nlp = \"trained_entity_linker\"\n",
        "path_output_kb = \"trained_entity_linker\"\n",
        "path_output_kb_vocab = \"trained_entity_linker\"\n",
        "\n",
        "\n",
        "#----\n",
        "# Load the dataset on the candidates\n",
        "# This contains their id, their name, a description, and aliases for their name\n",
        "\n",
        "def load_entities():\n",
        "    entities_loc = Path(path_candidates)\n",
        "\n",
        "    names = dict()\n",
        "    descriptions = dict()\n",
        "    aliases = dict()\n",
        "    with entities_loc.open(\"r\", encoding=\"utf8\") as csvfile:\n",
        "        csvreader = csv.reader(csvfile, delimiter=\",\")\n",
        "        next(csvreader) # Skip header row\n",
        "        for row in csvreader:\n",
        "            qid = row[0]\n",
        "            name = row[1]\n",
        "            desc = row[2]\n",
        "            alias = row[3]\n",
        "            names[qid] = name\n",
        "            descriptions[qid] = desc\n",
        "            aliases[qid] = alias\n",
        "    return names, descriptions, aliases\n",
        "\n",
        "# Create 3 dictionaries:\n",
        "# name_dict - ID -> name\n",
        "# desc_dict - ID -> description\n",
        "# aliases_dict - ID -> aliases\n",
        "name_dict, desc_dict, aliases_dict = load_entities()\n",
        "\n",
        "# Example content for Biden:\n",
        "print(f\"{'WMPID1289'}, name={name_dict['WMPID1289']}, \\\n",
        "    desc={desc_dict['WMPID1289']}, alias={aliases_dict['WMPID1289']}\")\n",
        "\n",
        "#----\n",
        "# Create a knowledge base\n",
        "# So far, information on these people sits in a set of dictionaries\n",
        "# Now we create a spacy knowledge base and populate it with the data above\n",
        "\n",
        "# Instantiate a knowledge base with 300-dimensional entity embedding\n",
        "# for spacy version above v3.5, instantiate the InMemoryLookupKB class instead of KnowledgeBase, which became an abstract class after v3.5\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=300)\n",
        "\n",
        "# Populate the knowledge base from the csv file\n",
        "# Starting with the id and description\n",
        "for qid, desc in desc_dict.items():\n",
        "    desc_doc = nlp(desc)\n",
        "    desc_enc = desc_doc.vector\n",
        "    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342) # 342 is an arbitrary value\n",
        "\n",
        "# Create a dictionary, with each unique alias as a key\n",
        "# and the value being the fecids of all the people with that alias as a list\n",
        "alias_to_fecids = dict()\n",
        "for qid, alias in aliases_dict.items():\n",
        "    for alias_specific in alias.split(\"|\"):\n",
        "        alias_to_fecids[alias_specific] = alias_to_fecids.get(alias_specific, []) + [qid]\n",
        "\n",
        "# Now, start adding aliases to the kb\n",
        "# The probabiltiy is 1/number of people with that alias\n",
        "for alias, fecids in alias_to_fecids.items():\n",
        "    kb.add_alias(alias=alias, entities=fecids, probabilities=[1/len(fecids) for fecid in fecids])\n",
        "\n",
        "# Create a list of entity ids (i.e. fec ids in our case) that is looped over later\n",
        "qids = name_dict.keys()\n",
        "kb.to_disk(path_intermediate_kb)\n",
        "\n",
        "\n",
        "#----\n",
        "df = pd.read_csv(path_training_samples, encoding = 'UTF-8')\n",
        "aliases = list(df['aliases'].str.split(\"|\"))\n",
        "\n",
        "# Apply NER to all training samples\n",
        "TRAIN_DOCS = []\n",
        "for text in tqdm(df['value']):\n",
        "    doc = nlp(text)\n",
        "    TRAIN_DOCS.append(doc)\n",
        "\n",
        "# Put the character indices in the data frame\n",
        "df['entity_start'] = np.nan\n",
        "df['entity_end'] = np.nan\n",
        "# Loop over the documents\n",
        "# and record the indices from the NER results in the df\n",
        "for d in range(len(TRAIN_DOCS)):\n",
        "    for entity in TRAIN_DOCS[d].ents:\n",
        "        if str(entity) in aliases[d]:\n",
        "            print([entity, entity.start_char, entity.end_char])\n",
        "            df.at[d, 'entity_start'] = entity.start_char\n",
        "            df.at[d, 'entity_end'] = entity.end_char\n",
        "            break\n",
        "\n",
        "\n",
        "# Get the indices of the rows of the data frame where an entity match was detected\n",
        "detected_entities_indices = np.where(df['entity_start'].isnull().to_numpy()==False)[0]\n",
        "detected_entities_indices = list(detected_entities_indices)\n",
        "\n",
        "# Make a new TRAIN_DOCS list with only those\n",
        "TRAIN_DOCS2 = [TRAIN_DOCS[i] for i in detected_entities_indices]\n",
        "print(\"Training on\", len(TRAIN_DOCS2), \"samples.\") #currently about 14k (out of 60k)\n",
        "\n",
        "# Create the annotations like this\n",
        "# {'links': {(39, 48): {'H8MO01143': 1.0}}}\n",
        "starts = [int(df['entity_start'][i]) for i in detected_entities_indices]\n",
        "ends = [int(df['entity_end'][i]) for i in detected_entities_indices]\n",
        "fecs = [df['wmpid'][i] for i in detected_entities_indices]\n",
        "# print(starts, ends, fecs)\n",
        "\n",
        "annotations = []\n",
        "for i in range(len(starts)):\n",
        "    annotations.append({'links': {(starts[i], ends[i]): {fecs[i]: 1.0}}, 'entities': [(starts[i], ends[i], 'PERSON')]})\n",
        "\n",
        "# Make another version of TRAIN_DOCS, this time making it the correct tuple again,\n",
        "#  with annotations as the second element\n",
        "TRAIN_DOCS3 = []\n",
        "for i in range(len(starts)):\n",
        "    TRAIN_DOCS3.append((TRAIN_DOCS2[i], annotations[i]))\n",
        "\n",
        "# Create gold-standard sentences\n",
        "if \"sentencizer\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"sentencizer\")\n",
        "sentencizer = nlp.get_pipe(\"sentencizer\")\n",
        "TRAIN_EXAMPLES = []\n",
        "for i in range(len(starts)):\n",
        "    example = Example.from_dict(nlp.make_doc(str(TRAIN_DOCS3[i][0])), annotations[i])\n",
        "    example.reference = sentencizer(example.reference)\n",
        "    TRAIN_EXAMPLES.append(example)\n",
        "\n",
        "# Initialize the entity linker component\n",
        "entity_linker = nlp.add_pipe(\"entity_linker\", config={\"incl_prior\": False}, last=True)\n",
        "entity_linker.initialize(get_examples=lambda: TRAIN_EXAMPLES, kb_loader=load_kb(path_intermediate_kb))\n",
        "\n",
        "# At this point, the untrained component already works\n",
        "test_doc = nlp(\"Donald Trump is a former president.\")\n",
        "for ent in test_doc.ents:\n",
        "    if ent.kb_id_ != 'NIL':\n",
        "        print(ent.kb_id_)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "loss_list = []\n",
        "with nlp.select_pipes(enable=[\"entity_linker\"]):   # train only the entity_linker\n",
        "    optimizer = nlp.resume_training() # This used to be begin_training, in spacy3 it seems it's resume because the component has already been initialized\n",
        "    optimizer.learn_rate = 0.001\n",
        "    for itn in tqdm(range(500)):   # one itn is one full pass over TRAIN_EXAMPLES\n",
        "        random.shuffle(TRAIN_EXAMPLES)\n",
        "        batches = minibatch(TRAIN_EXAMPLES, size=128)#size=compounding(4.0, 32.0, 1.001))  # increasing batch sizes -- seems to train WAY faster with a larger batch size (i.e. 128 rather than 32)\n",
        "        losses = {} #at the end of the epoch, this will contain the cumulative loss of all of its batches (and as far as I can tell, the loss for one batch is the mean loss of all the samples in it)\n",
        "        for batch in batches:\n",
        "            nlp.update(\n",
        "                batch,\n",
        "                drop=0.2,      # prevent overfitting\n",
        "                losses=losses,\n",
        "                sgd=optimizer,\n",
        "            )\n",
        "        #if itn % 50 == 0:\n",
        "        print(itn, \"Losses\", losses)   # print the training loss\n",
        "        loss_list.append(losses['entity_linker'])\n",
        "\n",
        "print(itn, \"Losses\", losses)\n",
        "\n",
        "# Save the nlp object to file\n",
        "nlp.to_disk(path_output_nlp)\n",
        "kb.to_disk(path_output_kb)\n",
        "kb.vocab.to_disk(path_output_kb_vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQlL8jsLzYVa"
      },
      "source": [
        "# inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UJEsAJbGAiX"
      },
      "source": [
        "Steps to take before attempting to run inference:\n",
        "\n",
        "- **Make sure that ```fb_2022_adid_text.csv.gz``` and ```trained_entity_linker``` are uploaded onto Google Drive. If they are not directly located in your main drive (e.g, they are in a folder) then the paths used in the code cells must be updated.**\n",
        "\n",
        "  - File ```fb_2022_adid_text.csv.gz``` will either already be present from running the train section, or else it must be downloaded from our Figshare page and then uploaded manually. You can get access using this [Data Access form](https://www.creativewmp.com/data-access/). If it is present as output from the train section, then you must update the path in the code cell as specified.\n",
        "\n",
        "  - Folder ```trained_entity_linker``` will either already be present as output from the train section, or else it must be downloaded from our Figshare page and then uploaded manually **as a zip file**. You can get access using this [Data Access form](https://www.creativewmp.com/data-access/). If it is present as output from the train section, then you must change the path in the code cell as specified.\n",
        "\n",
        "  - ***IF YOU ARE PART OF THE USABILITY STUDY YOU CAN SKIP THESE INSTRUCTIONS, AS YOU'LL DOWNLOAD THE FILES IN THE CODE.***\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Direct download of fb_2022_adid_text.csv.gz\n",
        "!wget -O fb_2022_adid_text.csv.gz https://figshare.com/ndownloader/files/49885527?private_link=c46dc366bbf4294a9be3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5WrTxvSnZT5",
        "outputId": "85e35771-53e0-4744-afe8-c7a8aa11a5f9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-02 18:44:51--  https://figshare.com/ndownloader/files/49885527?private_link=c46dc366bbf4294a9be3\n",
            "Resolving figshare.com (figshare.com)... 54.228.103.118, 34.252.194.191, 2a05:d018:1f4:d003:5690:ce60:6925:7a1e, ...\n",
            "Connecting to figshare.com (figshare.com)|54.228.103.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pstorage-wesleyan-1795779948/49885527/fb_2022_adid_text.csv.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA3OGA3B5WKAYJ2FVS/20241202/eu-west-1/s3/aws4_request&X-Amz-Date=20241202T184451Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=dd3554852d48c0cdc36ad029ca8a5eca971c45ad680664782f177b1ff0c97eb5 [following]\n",
            "--2024-12-02 18:44:51--  https://s3-eu-west-1.amazonaws.com/pstorage-wesleyan-1795779948/49885527/fb_2022_adid_text.csv.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA3OGA3B5WKAYJ2FVS/20241202/eu-west-1/s3/aws4_request&X-Amz-Date=20241202T184451Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=dd3554852d48c0cdc36ad029ca8a5eca971c45ad680664782f177b1ff0c97eb5\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.97.83, 52.218.41.99, 52.92.20.96, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.97.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 103037523 (98M) [application/gzip]\n",
            "Saving to: ‘fb_2022_adid_text.csv.gz’\n",
            "\n",
            "fb_2022_adid_text.c 100%[===================>]  98.26M  27.5MB/s    in 3.6s    \n",
            "\n",
            "2024-12-02 18:44:55 (27.5 MB/s) - ‘fb_2022_adid_text.csv.gz’ saved [103037523/103037523]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8j5HV0-EeUqc",
        "outputId": "319fb6ce-a999-4968-c803-7a54ac94b96f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: data.table 1.16.2 using 1 threads (see ?getDTthreads).  \n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Latest news: r-datatable.com\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "Attaching package: ‘dplyr’\n",
            "\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The following objects are masked from ‘package:data.table’:\n",
            "\n",
            "    between, first, last\n",
            "\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    filter, lag\n",
            "\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The following objects are masked from ‘package:base’:\n",
            "\n",
            "    intersect, setdiff, setequal, union\n",
            "\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: |--------------------------------------------------|\n",
            "|\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: |\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "%%R\n",
        "\n",
        "library(data.table)\n",
        "library(dplyr)\n",
        "library(tidyr)\n",
        "\n",
        "setwd(\"./\")\n",
        "\n",
        "# Input files\n",
        "# This is an output from data-post-production/01-merge-results/01_merge_preprocessed_results.\n",
        "# Make sure that this file is in the colab environment before attempting to run\n",
        "\n",
        "# If you ran the train, this path must be changed to \"/content/fb_2022_adid_text.csv.gz\"\n",
        "path_ads <- \"/content/fb_2022_adid_text.csv.gz\"\n",
        "# Output files\n",
        "path_prepared_ads <- \"inference_all_fb22_ads.csv.gz\"\n",
        "\n",
        "# Ads\n",
        "df <- fread(path_ads, encoding = \"UTF-8\")\n",
        "\n",
        "# Subset to clean text dataframe\n",
        "df2 <- df %>%\n",
        "  select(\n",
        "    ad_id, google_asr_text, page_name, disclaimer, ad_creative_body,\n",
        "    ad_creative_link_title, ad_creative_link_description,\n",
        "    aws_ocr_text_img, aws_ocr_text_vid, ad_creative_link_caption\n",
        "  )\n",
        "\n",
        "# Aggregate\n",
        "df3 <- df2 %>%\n",
        "  pivot_longer(-ad_id) %>%\n",
        "  filter(value != \"\") %>%\n",
        "  mutate(id = paste(ad_id, name, sep = \"__\")) %>%\n",
        "  select(-c(ad_id, name))\n",
        "\n",
        "df3 <- aggregate(df3$id, by = list(df3$value), c)\n",
        "names(df3) <- c(\"text\", \"id\")\n",
        "\n",
        "# Save\n",
        "fwrite(df3, path_prepared_ads)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pnd2zRqTjdPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e78e52-d249-4f3d-93f5-594f7dca2487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__MACOSX folder removed.\n"
          ]
        }
      ],
      "source": [
        "# Direct download of trained_entity_linker.zip\n",
        "\n",
        "# Download the file\n",
        "!wget -O trained_entity_linker.zip https://figshare.wesleyan.edu/ndownloader/files/46512400\n",
        "\n",
        "# Unzip the file\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_path = \"trained_entity_linker.zip\"\n",
        "\n",
        "# Extract the contents\n",
        "!unzip trained_entity_linker.zip\n",
        "\n",
        "# Remove the __MACOSX folder if it exists\n",
        "if os.path.exists(\"__MACOSX\"):\n",
        "    !rm -rf __MACOSX/\n",
        "    print(\"__MACOSX folder removed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jL_dPqk3gPmf",
        "outputId": "8e2eb033-265a-4a76-d41e-24960144a6a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "100%|██████████| 500/500 [00:32<00:00, 15.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows: 100%|██████████| 320/320 [00:00<00:00, 3854.73it/s]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import spacy # Use version '3.2.4'\n",
        "# Make sure that this file is in the colab environment before attempting to run\n",
        "# If you ran the knowledge_base section, this path must be changed to \"/content/trained_entity_linker.zip\"\n",
        "nlp = spacy.load(\"/content/trained_entity_linker\") # trained_entity_linker is output from 02_train_entity_linking.py\n",
        "from spacy.kb import KnowledgeBase #vscode pylinter complains about this, but it actually loads fine\n",
        "from spacy.util import minibatch, compounding\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Input files\n",
        "path_prepared_ads = \"/content/inference_all_fb22_ads.csv.gz\"\n",
        "# Output files\n",
        "path_el_results = \"entity_linking_results_fb22.csv.gz\"\n",
        "path_el_results_notext = \"entity_linking_results_fb22_notext.csv.gz\"\n",
        "\n",
        "# Read in prepared ads\n",
        "df = pd.read_csv(path_prepared_ads)\n",
        "\n",
        "# Code below runs a random sample of rows from the input dataframe,\n",
        "# where n equals the # of rows\n",
        "\n",
        "df = df.sample(n=500)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "df = df.replace(np.nan, '', regex=True)\n",
        "fields = ['text']\n",
        "\n",
        "\n",
        "def get_sims(sent_emb, ent_id):\n",
        "\n",
        "    sentence_encoding = sent_emb\n",
        "    entity_encodings = np.asarray(nlp.get_pipe('entity_linker').kb.get_vector(ent_id))\n",
        "\n",
        "    sentence_norm = np.linalg.norm(sentence_encoding, axis=0)\n",
        "    entity_norm = np.linalg.norm(entity_encodings, axis=0)\n",
        "\n",
        "    sims = np.dot(entity_encodings, sentence_encoding) / (sentence_norm * entity_norm)\n",
        "\n",
        "    return(sims)\n",
        "\n",
        "# Give non-candidates like Kamala Harris a boost in comparison to actual cands\n",
        "# This is necessary because non-cands don't have much training data, so the model\n",
        "# almost never picks them\n",
        "def is_it_kamala(nlpd_doc, possible_cands, likely_cand, boost_size = 0.1):\n",
        "\n",
        "    sent_emb = nlpd_doc.vector\n",
        "\n",
        "    sims = []\n",
        "    for h in possible_cands:\n",
        "\n",
        "        sim = get_sims(sent_emb, h)\n",
        "        if h == likely_cand:\n",
        "            sim += boost_size\n",
        "\n",
        "        sims.append(sim)\n",
        "\n",
        "    picked_cand = np.array(sims).argmax()\n",
        "    picked_cand_id = possible_cands[picked_cand]\n",
        "\n",
        "    return(picked_cand_id)\n",
        "\n",
        "harrises = ['WMPID1144',\n",
        "            'WMPID3207',\n",
        "            'WMPID2']\n",
        "\n",
        "barretts = ['WMPID3995',\n",
        "            'WMPID17']\n",
        "\n",
        "\n",
        "# This loop can take anywhere from 6-8 hours.\n",
        "\n",
        "for f in fields:\n",
        "\n",
        "    entities_in_field = []\n",
        "    entities_in_field_start = []\n",
        "    entities_in_field_end = []\n",
        "\n",
        "    for i in tqdm(range(len(df))):\n",
        "\n",
        "        entities_in_ad = []\n",
        "        entities_in_ad_start = []\n",
        "        entities_in_ad_end = []\n",
        "\n",
        "        if pd.isnull(df[f][i])==False:\n",
        "            test_text = df[f][i]\n",
        "            test_doc = nlp(test_text)\n",
        "            for ent in test_doc.ents:\n",
        "                if ent.kb_id_ != 'NIL':\n",
        "\n",
        "                    # Make sure we don't misclassify House as Steve House\n",
        "                    # Steve House didn't run in 2022 \\o/ yay!\n",
        "                    # if (ent.kb_id_ == 'H0CO06119') & (ent.label_ == 'ORG'):\n",
        "                    #     pass\n",
        "\n",
        "                    # Make sure we don't misclassify Kamala as one of the other Harrises\n",
        "                    if ent.kb_id_ in harrises:\n",
        "                        # Check if it is actually Kamala\n",
        "                        harrises_cand = is_it_kamala(test_doc, harrises, 'WMPID2', boost_size = 0.16)\n",
        "                        entities_in_ad.append(harrises_cand)\n",
        "                        entities_in_ad_start.append(ent.start_char)\n",
        "                        entities_in_ad_end.append(ent.end_char)\n",
        "\n",
        "                    # Make sure we don't misclassify Amy Coney Barrett as Thomas More Barrett\n",
        "                    # If the EL detects Thomas More Barrett\n",
        "                    elif ent.kb_id_ == 'WMPID3995':\n",
        "                        # Check if it is actually Amy Coney\n",
        "                        barretts_cand = is_it_kamala(test_doc, barretts, 'WMPID17', boost_size = 0.17)\n",
        "                        entities_in_ad.append(barretts_cand)\n",
        "                        entities_in_ad_start.append(ent.start_char)\n",
        "                        entities_in_ad_end.append(ent.end_char)\n",
        "\n",
        "                    # If it is none of these, proceed as normal\n",
        "                    else:\n",
        "                        entities_in_ad.append(ent.kb_id_)\n",
        "                        entities_in_ad_start.append(ent.start_char)\n",
        "                        entities_in_ad_end.append(ent.end_char)\n",
        "\n",
        "        entities_in_field.append(entities_in_ad)\n",
        "        entities_in_field_start.append(entities_in_ad_start)\n",
        "        entities_in_field_end.append(entities_in_ad_end)\n",
        "\n",
        "\n",
        "    df[f + '_detected_entities'] = entities_in_field\n",
        "    df[f + '_start'] = entities_in_field_start\n",
        "    df[f + '_end'] = entities_in_field_end\n",
        "\n",
        "    print(f, \"done!\")\n",
        "\n",
        "## Prepare data for additional dictionary search for Trump and Biden only on disclaimer and page name fields\n",
        "# Split ids\n",
        "df['id'] = df['id'].str.split('|')\n",
        "# \"Un-deduplicate\", or \"Re-hydrate\", in WMP lingo\n",
        "df = df.explode('id')\n",
        "# Split into ad id and field\n",
        "df_ids = df['id'].str.split('__', expand = True)\n",
        "df_ids.columns = ['ad_id', 'field']\n",
        "df = pd.concat([df, df_ids], axis = 1)\n",
        "df = df.drop(labels = ['id'], axis = 1)\n",
        "# Split the data frame into disclaimer/page_name, and other\n",
        "df_1 = df[df['field'].isin(['disclaimer', 'page_name'])]\n",
        "df_2 = df[df['field'].isin(['disclaimer', 'page_name']) == False]\n",
        "# Make a copy of df_1\n",
        "df1 = df_1.copy()\n",
        "df1.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# This function does a simple dictionary search on disclaimer and page name fields.\n",
        "# It only does this search for Biden and Trump.\n",
        "# If this dictionary search finds any entity that was not detected by the model, it adds the corresponding WMPID to the detected entities list.\n",
        "\n",
        "def update_detected_entities(df):\n",
        "    # Mapping of names to their corresponding ids\n",
        "    name_to_id = {'biden': 'WMPID1289', 'trump': 'WMPID1290'}\n",
        "    # Iterate over each row in the DataFrame with tqdm\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
        "        # Split the text_detected_entities column to a list\n",
        "        detected_entities = row['text_detected_entities']\n",
        "\n",
        "        # Initialize lists to store start and end indices\n",
        "        start_indices = row['text_start']\n",
        "        end_indices = row['text_end']\n",
        "\n",
        "        # Convert the text to lowercase\n",
        "        text = row['text'].lower()\n",
        "\n",
        "        # Iterate over each name to be detected\n",
        "        for name in name_to_id.keys():\n",
        "            # Find all occurrences of the name in the text\n",
        "            name_occurrences = [i for i in range(len(text)) if text.startswith(name, i)]\n",
        "\n",
        "            # Check each occurrence of the name\n",
        "            for start_index in name_occurrences:\n",
        "                # Check if the name is already detected by the entity linking model\n",
        "                already_detected = False\n",
        "                for start, end in zip(start_indices, end_indices):\n",
        "                    if start <= start_index < end:\n",
        "                        already_detected = True\n",
        "                        break\n",
        "\n",
        "                # If the name is not already detected, add its ID\n",
        "                if not already_detected:\n",
        "                    end_index = start_index + len(name)\n",
        "                    detected_entities.append(name_to_id[name])\n",
        "                    start_indices.append(start_index)\n",
        "                    end_indices.append(end_index)\n",
        "\n",
        "        # Update the DataFrame with the modified lists\n",
        "        df.at[index, 'text_detected_entities'] = detected_entities\n",
        "        df.at[index, 'text_start'] = start_indices\n",
        "        df.at[index, 'text_end'] = end_indices\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df2 = update_detected_entities(df1)\n",
        "\n",
        "\n",
        "# Recombine the dataframes\n",
        "df = pd.concat([df2, df_2], axis = 0)\n",
        "\n",
        "# Save results\n",
        "df.to_csv(path_el_results, index=False)\n",
        "df = df.drop(['text'], axis = 1)\n",
        "df.to_csv(path_el_results_notext, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8yKaYoRYj3Jt"
      },
      "outputs": [],
      "source": [
        "# Post-processing for the entity linking results\n",
        "# Gather up all detected entities from different fields and put them all together\n",
        "\n",
        "%%R\n",
        "\n",
        "library(data.table)\n",
        "library(dplyr)\n",
        "library(tidyr)\n",
        "library(stringr)\n",
        "\n",
        "setwd(\"./\")\n",
        "\n",
        "# Paths\n",
        "# In\n",
        "path_detected_entities <- \"/content/entity_linking_results_fb22_notext.csv.gz\"\n",
        "# Out\n",
        "path_finished_enties <- \"detected_entities_fb22.csv.gz\"\n",
        "path_finished_enties_for_ad_tone <- \"detected_entities_fb22_for_ad_tone.csv.gz\"\n",
        "\n",
        "# Read in Spacy's detected entities\n",
        "el <- fread(path_detected_entities)\n",
        "\n",
        "# Transform the Python-based detected entities field into an R list\n",
        "transform_pylist <- function(x) {\n",
        "  x <- str_remove_all(x, \"\\\\[|\\\\]|\\\\'\")\n",
        "  x <- str_remove_all(x, \" \")\n",
        "  return(x)\n",
        "}\n",
        "el$text_detected_entities <- transform_pylist(el$text_detected_entities)\n",
        "# Remove all ads with no detected entities\n",
        "el <- el %>% filter(text_detected_entities != \"\")\n",
        "# For ad tone, remove disclaimer and page_name\n",
        "el_at <- el %>% filter(!field %in% c(\"page_name\", \"disclaimer\"))\n",
        "# Aggregate over fields, then clean up and put things back into a list\n",
        "el <- aggregate(el$text_detected_entities, by = list(el$ad_id), c)\n",
        "el$x <- lapply(el$x, paste, collapse = \",\")\n",
        "el$x <- str_split(el$x, \",\")\n",
        "names(el) <- c(\"ad_id\", \"detected_entities\")\n",
        "# Same for ad tone\n",
        "el_at <- aggregate(el_at$text_detected_entities, by = list(el_at$ad_id), c)\n",
        "el_at$x <- lapply(el_at$x, paste, collapse = \",\")\n",
        "el_at$x <- str_split(el_at$x, \",\")\n",
        "names(el_at) <- c(\"ad_id\", \"detected_entities\")\n",
        "\n",
        "# Save version with combined fields\n",
        "fwrite(el, path_finished_enties)\n",
        "fwrite(el_at, path_finished_enties_for_ad_tone)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FPtw0zahIaE6",
        "outputId": "4a3e0288-f947-4fc8-fa0b-57ac4997910c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 ad_id                                  detected_entities\n",
              "0   x_1014277789264163  WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4...\n",
              "1   x_1023768478192159                                          WMPID5292\n",
              "2   x_1025448931468871                                          WMPID5292\n",
              "3   x_1027022148014880                                          WMPID5292\n",
              "4   x_1030271714332789                                          WMPID5292\n",
              "5   x_1033176647384254                                          WMPID5311\n",
              "6   x_1036431663721346                                          WMPID5292\n",
              "7   x_1040820059962925                            WMPID67|WMPID67|WMPID67\n",
              "8   x_1041128443250119                                          WMPID5311\n",
              "9   x_1051179346280418                                          WMPID5344\n",
              "10  x_1051467328855853                                          WMPID5305\n",
              "11  x_1051742615534803                                          WMPID2130\n",
              "12  x_1054620845247996                                          WMPID2065\n",
              "13  x_1060448521314417                                          WMPID4990\n",
              "14  x_1062237487780408                                          WMPID5344\n",
              "15  x_1064218880934102                                          WMPID5344\n",
              "16  x_1067500453914932                                          WMPID5344\n",
              "17  x_1074583689844428                                          WMPID5344\n",
              "18  x_1074657953424082                      WMPID1289|WMPID1289|WMPID1289\n",
              "19  x_1075038766534341                                WMPID1450|WMPID1450\n",
              "20  x_1077446949622882                                          WMPID4009\n",
              "21  x_1080701729229421                      WMPID5222|WMPID5196|WMPID1481\n",
              "22  x_1081451192567203                                           WMPID987\n",
              "23  x_1082137692470104  WMPID4650|WMPID1450|WMPID4650|WMPID1450|WMPID1...\n",
              "24  x_1086712102206761                                          WMPID5344\n",
              "25  x_1088671931850265                                          WMPID5344\n",
              "26  x_1089362775072718                                          WMPID4009\n",
              "27  x_1090427978288841  WMPID2037|WMPID2037|WMPID1465|WMPID1459|WMPID1...\n",
              "28  x_1090478871592829                                          WMPID5292\n",
              "29  x_1093230624725959                                          WMPID5344\n",
              "30  x_1095252684518032                                          WMPID5344\n",
              "31  x_1096565294394833                                          WMPID4009\n",
              "32  x_1096989187590050                      WMPID5281|WMPID5323|WMPID1290\n",
              "33  x_1098102500910432                                          WMPID2987\n",
              "34  x_1100738814165142                                          WMPID2130\n",
              "35  x_1105351973519549                                  WMPID160|WMPID160\n",
              "36  x_1105550523668046                                          WMPID5292\n",
              "37  x_1105587616810201                                          WMPID5311\n",
              "38  x_1106751636896898                                 WMPID1321|WMPID787\n",
              "39  x_1107689059881792                                          WMPID5292\n",
              "40  x_1109288729976973                                          WMPID1029\n",
              "41  x_1113532509268121  WMPID4650|WMPID1450|WMPID4650|WMPID1450|WMPID1...\n",
              "42  x_1114228522803027                                          WMPID5292\n",
              "43  x_1114363869247888                                          WMPID5292\n",
              "44  x_1115216699393133                      WMPID5281|WMPID5323|WMPID1290\n",
              "45  x_1115703942635399                                          WMPID2130\n",
              "46  x_1119118438993066                                          WMPID1790\n",
              "47  x_1120105118893510                                WMPID4904|WMPID4904\n",
              "48  x_1121529635233861                                          WMPID5344\n",
              "49  x_1124609751482869                                          WMPID5292"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8b1c708-083d-4235-bdfc-4ec51afb36e1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ad_id</th>\n",
              "      <th>detected_entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>x_1014277789264163</td>\n",
              "      <td>WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>x_1023768478192159</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>x_1025448931468871</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>x_1027022148014880</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>x_1030271714332789</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>x_1033176647384254</td>\n",
              "      <td>WMPID5311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>x_1036431663721346</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>x_1040820059962925</td>\n",
              "      <td>WMPID67|WMPID67|WMPID67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>x_1041128443250119</td>\n",
              "      <td>WMPID5311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>x_1051179346280418</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>x_1051467328855853</td>\n",
              "      <td>WMPID5305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>x_1051742615534803</td>\n",
              "      <td>WMPID2130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>x_1054620845247996</td>\n",
              "      <td>WMPID2065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>x_1060448521314417</td>\n",
              "      <td>WMPID4990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>x_1062237487780408</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>x_1064218880934102</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>x_1067500453914932</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>x_1074583689844428</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>x_1074657953424082</td>\n",
              "      <td>WMPID1289|WMPID1289|WMPID1289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>x_1075038766534341</td>\n",
              "      <td>WMPID1450|WMPID1450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>x_1077446949622882</td>\n",
              "      <td>WMPID4009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>x_1080701729229421</td>\n",
              "      <td>WMPID5222|WMPID5196|WMPID1481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>x_1081451192567203</td>\n",
              "      <td>WMPID987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>x_1082137692470104</td>\n",
              "      <td>WMPID4650|WMPID1450|WMPID4650|WMPID1450|WMPID1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>x_1086712102206761</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>x_1088671931850265</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>x_1089362775072718</td>\n",
              "      <td>WMPID4009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>x_1090427978288841</td>\n",
              "      <td>WMPID2037|WMPID2037|WMPID1465|WMPID1459|WMPID1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>x_1090478871592829</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>x_1093230624725959</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>x_1095252684518032</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>x_1096565294394833</td>\n",
              "      <td>WMPID4009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>x_1096989187590050</td>\n",
              "      <td>WMPID5281|WMPID5323|WMPID1290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>x_1098102500910432</td>\n",
              "      <td>WMPID2987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>x_1100738814165142</td>\n",
              "      <td>WMPID2130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>x_1105351973519549</td>\n",
              "      <td>WMPID160|WMPID160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>x_1105550523668046</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>x_1105587616810201</td>\n",
              "      <td>WMPID5311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>x_1106751636896898</td>\n",
              "      <td>WMPID1321|WMPID787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>x_1107689059881792</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>x_1109288729976973</td>\n",
              "      <td>WMPID1029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>x_1113532509268121</td>\n",
              "      <td>WMPID4650|WMPID1450|WMPID4650|WMPID1450|WMPID1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>x_1114228522803027</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>x_1114363869247888</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>x_1115216699393133</td>\n",
              "      <td>WMPID5281|WMPID5323|WMPID1290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>x_1115703942635399</td>\n",
              "      <td>WMPID2130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>x_1119118438993066</td>\n",
              "      <td>WMPID1790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>x_1120105118893510</td>\n",
              "      <td>WMPID4904|WMPID4904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>x_1121529635233861</td>\n",
              "      <td>WMPID5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>x_1124609751482869</td>\n",
              "      <td>WMPID5292</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8b1c708-083d-4235-bdfc-4ec51afb36e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a8b1c708-083d-4235-bdfc-4ec51afb36e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a8b1c708-083d-4235-bdfc-4ec51afb36e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bb585add-35d5-4f62-9596-5a5c9e414628\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bb585add-35d5-4f62-9596-5a5c9e414628')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bb585add-35d5-4f62-9596-5a5c9e414628 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 781,\n  \"fields\": [\n    {\n      \"column\": \"ad_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 781,\n        \"samples\": [\n          \"x_655495639246149\",\n          \"x_649397256295596\",\n          \"x_626245025649076\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"detected_entities\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 87,\n        \"samples\": [\n          \"WMPID1289|WMPID5206\",\n          \"WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4650|WMPID4650\",\n          \"WMPID1290\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Examine results\n",
        "\n",
        "data = pd.read_csv('/content/detected_entities_fb22.csv.gz')\n",
        "data.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NACw73q16xSQ"
      },
      "source": [
        "# Results Analysis Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8CqQ-C47F9c"
      },
      "source": [
        "Steps to take before attempting to run:\n",
        "\n",
        "- **Make sure that ```readcsv.py``` and any data that you want to analyze are uploaded onto Google Drive. If they are not directly located in your main drive (e.g, they are in a folder) then the paths used in the code cells must be updated.**\n",
        "\n",
        "  - File [```readcsv.py```](https://github.com/Wesleyan-Media-Project/entity_linking_2022_usabilitystudy/blob/main/analysis/readcsv.py) can be downloaded/uploaded manually using the link provided.\n",
        "  - [Data](https://github.com/Wesleyan-Media-Project/entity_linking_2022_usabilitystudy/tree/main/facebook/data) will either already be present as output from running various sections of this notebook, or you can download/upload files directly from the GitHub repository from the link provided. If you upload data manually, then the path must be changed accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOBzsGSmjGFJ",
        "outputId": "ea342c5b-95dc-4b40-e750-e50e18097465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: readcsv.py [-h] --file FILE [--skiprows SKIPROWS] [--nrows NROWS]\n",
            "                  [--filter_text FILTER_TEXT]\n",
            "\n",
            "Filter large CSV files and save results to multiple Excel files if necessary.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --file FILE           Path to the CSV file.\n",
            "  --skiprows SKIPROWS   Number of rows to skip at the start of the file.\n",
            "  --nrows NROWS         Number of rows to read from the file. Read 10000 rows if not specified.\n",
            "  --filter_text FILTER_TEXT\n",
            "                        Text to filter the rows.\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/readcsv.py --h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htDAvUCd61jq",
        "outputId": "0b640220-ee33-4b53-e45f-3cd7d40d0721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input file: /content/entity_linking_results_fb22.csv.gz\n",
            "Start reading data from row: 0\n",
            "Number of rows to read: 10000\n",
            "Filter text: No filter is applied\n",
            "Filtered data saved to: Readcsv_Output_20241130_164043.xlsx\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/readcsv.py --file /content/entity_linking_results_fb22.csv.gz"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uw1nuufm0ZrV"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}